{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook was originally ran on Google Colab, local running will need modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7u4ECvI4DWd0"
   },
   "outputs": [],
   "source": [
    "# === Imports === #\n",
    "\n",
    "# My imports\n",
    "import os\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "# Various python packages are used in this notebook. Please get yourself used to them (optional).\n",
    "import pandas as pd  # used for storing a tabular representation of the dataset, similar to XLS files.\n",
    "from pathlib import Path # used to check if the saved model files and accessories.\n",
    "import requests #used to request remote judge.csv evaluation \n",
    "from sklearn.preprocessing import StandardScaler  # used for normalization of dataset\n",
    "from sklearn.preprocessing   import LabelBinarizer    # used for splitting the gender column\n",
    "from sklearn.preprocessing   import MinMaxScaler      # used for normalization of dataset\n",
    "from sklearn.model_selection import train_test_split  # used for performing the train-test split of a dataframe\n",
    "import cv2                                            # OpenCV used for image processing\n",
    "import random   #random number generator\n",
    "import datetime #used to get current date/time\n",
    "import math     #math/numerical functions\n",
    "import os       #os specific functions, like file open/close etc.\n",
    "import gc       #garbage collection module -- used to manually clean up memory spaces/references.\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder   #My favorite categorical to numerical feature conversion tool\n",
    "from tensorflow import keras  # keras used for construction of the Artificial neural network\n",
    "from keras.models import Sequential #keras model architectures\n",
    "from keras.layers import Conv2D, MaxPool2D, Dense, Flatten,Dropout, BatchNormalization #types of layers\n",
    "from keras.losses import mean_squared_error, huber, log_cosh  #built-in loss \n",
    "from tensorflow.python.keras.saving import hdf5_format  #used for saving models \n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard  #callbacks\n",
    "from keras.models import model_from_json  #used for loading model architecture from json file\n",
    "import h5py  #saved model type\n",
    "\n",
    "import matplotlib.pyplot as plt  # used for training visualization\n",
    "import numpy as np  # numpy arrays used for matrix computations\n",
    "\n",
    "# === Extra Configurations for the GPU Environment === #\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "if len(physical_devices)>0: #If you have at least one \"configured\" GPU, let's use it; otherwise, pass\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XPEVzFLFFfC9",
    "outputId": "cec2a656-c404-410b-b147-618cc759215e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on Google colab...\n",
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Setting work environment with dataset. If on Google colaboratory, we need to extract dataset stored in google drive,\n",
    "otherwise the dataset is already there.\n",
    "\"\"\"\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    print('Running on Google colab...')\n",
    "    drive.mount('/content/drive')\n",
    "    #import zipfile\n",
    "    #with zipfile.ZipFile('/content/drive/MyDrive/Colab Notebooks/csci5931sp22/PA2-dataset/Archive.zip', 'r') as zip_ref:\n",
    "    #    zip_ref.extractall('dataset/')\n",
    "except:\n",
    "    print('Running on local machine...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I3UFypzDFGlC",
    "outputId": "007b26a1-67f9-48b7-d20e-c0b6560cb90d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 0, 0]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_path = './drive/MyDrive/Colab Notebooks/DAiSEE/'\n",
    "\n",
    "#dataset = os.listdir('DataSet')\n",
    "all_labels = pd.read_csv(os.path.join(folder_path, 'Labels/AllLabels.csv'))\n",
    "\n",
    "file_extension = '.avi'\n",
    "sample_num = '5000441001' + file_extension\n",
    "\n",
    "all_labels[all_labels['ClipID']==sample_num].values.tolist()[0][-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jcLBGMs7I6-i",
    "outputId": "d1ed5380-0e27-4862-cde5-20c90f86e173"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [28:04<00:00, 80.21s/it] \n",
      "100%|██████████| 70/70 [2:17:46<00:00, 118.09s/it]\n",
      "100%|██████████| 22/22 [40:24<00:00, 110.19s/it]\n"
     ]
    }
   ],
   "source": [
    "subset_directory = os.path.join(folder_path, 'DataSet')\n",
    "subsets = ['Test', 'Train', 'Validation']\n",
    "\n",
    "generated_folder = 'gen-full-4'\n",
    "resize_scale = \"299:224\"\n",
    "\n",
    "if generated_folder in os.listdir(folder_path):\n",
    "    shutil.rmtree(os.path.join(folder_path, generated_folder))\n",
    "os.makedirs(os.path.join(folder_path, generated_folder))\n",
    "target_directory = os.path.join(folder_path, generated_folder)\n",
    "\n",
    "for subset in subsets:\n",
    "    subset_path = os.path.join(subset_directory, subset)\n",
    "\n",
    "    os.makedirs(os.path.join(folder_path, generated_folder, subset))\n",
    "\n",
    "    persons = os.listdir(subset_path)\n",
    "    for person in tqdm(persons):\n",
    "        person_path = os.path.join(subset_path, person)\n",
    "        person_videos = os.listdir(person_path)\n",
    "        for person_video in person_videos:\n",
    "            try:\n",
    "\n",
    "                person_video_path = os.path.join(person_path, person_video)\n",
    "                \n",
    "                if len(os.listdir(person_video_path)) == 0:\n",
    "                    continue\n",
    "\n",
    "                filename = os.listdir(person_video_path)[0]\n",
    "                file_path = os.path.join(person_video_path, filename)\n",
    "                \n",
    "                new_filename = person_video + \"-%d.jpg\"\n",
    "\n",
    "                target_path = os.path.join(target_directory, subset, new_filename)\n",
    "\n",
    "                '''\n",
    "                Gen2 = 2 hz = 0.5 seconds between frames\n",
    "                Gen3 = .5 hz = 2 seconds between frames\n",
    "                '''\n",
    "\n",
    "                command = 'ffmpeg -i \"' + file_path + '\" -vf scale=' + resize_scale + ' -r 0.5 -qscale:v 2 \"' + target_path + '\"'\n",
    "                os.system(command)\n",
    "            except:\n",
    "                print(\"error with \" + person_video)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gHmjEGkbupGC",
    "outputId": "e92ac07f-9ae0-489b-8e80-4cd823a0e7c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success...\n"
     ]
    }
   ],
   "source": [
    "print(\"Success...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8IVBq5JxguE"
   },
   "source": [
    "https://medium.com/analytics-vidhya/write-your-own-custom-data-generator-for-tensorflow-keras-1252b64e41c3"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
